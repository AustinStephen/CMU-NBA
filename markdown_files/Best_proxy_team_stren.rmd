---
title: "Creating the Best Team Strength Proxy"
author: "Austin Stephen"
date: "7/22/2021"
output: html_document
---
```{r include = FALSE}
library(nbastatR)
library(NBAr)
library(ballr)
library(tidyverse)
library(dplyr)
library(readr)
knitr::opts_chunk$set(
  cache = TRUE,
  echo = FALSE,
  message = FALSE, 
  warning = FALSE,
  fig.height=3
)

together_net_Rating_allWindows <- read_csv("../data/proxy_team_strength/clean_team_strength_columns.csv")


```
## Two Approachs to Building a Proxy for Team Strength 

1. __Constrained:__ Limited to the information available to a team when they would 
have played the game.
    * Model uses the temporally weighted last 40 games played outcomes.
    * This model explains 13.51 percent of the variance in the response.
    
The baseline constrained model uses the rolling win percentage difference
between the two teams. This baseline model explains 11.75 percent of the 
variance in the response. We achieve a __20 percent improvement__ over this model.  

2. __Unconstrained:__ All available information about the team not
incorporated into the response is used to break down what contributed to the 
outcome of a given game.
    * The model uses the outcome 30 previous games and 30 games into the future
temporally weighted, and the season average net rating difference to 
get a more precise understanding of the teams strength at a given game.
    * This model explains 17.49 percent of the variation in the response.
    * This is especially powerful in extracting information from early season 
games because the constrained model would be only have a few samples to 
observe the teams performance.  

The baseline unconstrained model uses the rolling win 
percentage difference between teams and their average rating difference for 
the end of the season. This model explains 14.43 percent of the variance in the 
response. We achieve a __28 percent improvement__ over this model.

---
### Model Details:

### Hyperparameter tunning for the window size:

```{r fig.height= 3}
# ## Running win percent diff
summary(lm(score_diff ~ net_rating_diff , data = together_net_Rating_allWindows))
# summary(lm(score_diff ~ win_percent_diff, data = together_net_Rating_allWindows))
# #Adjusted R-squared:0.1182 => 0.2302 

# ## Window size of 2
# summary(lm(score_diff ~ wind_2, data = together_net_Rating_allWindows))
# summary(lm(score_diff ~ wind_2 + net_rating_diff , data = together_net_Rating_allWindows))
# #Adjusted R-squared:  0.04932 =>  0.2302
# 
# 
# ## Window size of 5
# summary(lm(score_diff ~ wind_5, data = together_net_Rating_allWindows))
# summary(lm(score_diff ~ wind_5 + net_rating_diff , data = together_net_Rating_allWindows))
# #Adjusted R-squared:  0.09425  => 0.2305 
# 
# ## Window size of 7
# summary(lm(score_diff ~ wind_7, data = together_net_Rating_allWindows))
# summary(lm(score_diff ~ wind_7 + net_rating_diff , data = together_net_Rating_allWindows))
# # Adjusted R-squared:  0.111 => 0.2304  
# 
# ## Window size of 10
# summary(lm(score_diff ~ wind_10, data = together_net_Rating_allWindows))
# summary(lm(score_diff ~ wind_10 + net_rating_diff , data = together_net_Rating_allWindows))
# # Adjusted R-squared: 0.1237 => 0.2302 
# 
# 
# ## Window size of 15
# summary(lm(score_diff ~ wind_15, data = together_net_Rating_allWindows))
# summary(lm(score_diff ~ wind_15 + net_rating_diff , data = together_net_Rating_allWindows))
# # Adjusted R-squared: 0.1393 => 0.2302 
# 
# ## Window size of 20
# summary(lm(score_diff ~ wind_20, data = together_net_Rating_allWindows))
# summary(lm(score_diff ~ wind_20 + net_rating_diff , data =
# together_net_Rating_allWindows))
# # Adjusted R-squared: 0.1449 => 0.2303   
# 
# ## Window size of 25
# summary(lm(score_diff ~ wind_25, data = together_net_Rating_allWindows))
# summary(lm(score_diff ~ wind_25 + net_rating_diff , data =
# together_net_Rating_allWindows))
# # Adjusted R-squared: 0.1463  => 0.2307  
# 
# ## Window size of 30
# summary(lm(score_diff ~ wind_30, data = together_net_Rating_allWindows))
# summary(lm(score_diff ~ wind_30 + net_rating_diff , data =
# together_net_Rating_allWindows))
# #Adjusted R-squared: 0.1468  => 0.2307  
# #
# # # ## Window size of 35
# summary(lm(score_diff ~ wind_35, data = together_net_Rating_allWindows))
# summary(lm(score_diff ~ wind_35 + net_rating_diff , data =
# together_net_Rating_allWindows))
# #  Adjusted R-squared: 0.1487  =>  0.2313 
# #
# # # ## Window size of 40
# summary(lm(score_diff ~ wind_40, data = together_net_Rating_allWindows))
# summary(lm(score_diff ~ wind_40 + net_rating_diff , data = together_net_Rating_allWindows))
# # # # Adjusted R-squared: 0.1483 =>  0.2315

r_squared_window <- c(0.1182, 0.04932, 0.09425, 0.111, 0.1237, 0.1393, 0.1449,
                      0.1463, 0.1468, 0.1487, 0.1483)

r_squared_window_plus_sn_avg <- c(0.2302 , 0.2302, 0.2305, 0.2304, 0.2302, 0.2302,
                                  0.2303, 0.2307, 0.2307, 0.2313, 0.2315)

diff <- r_squared_window_plus_sn_avg - r_squared_window

window_size <- c("roll_win_pct_diff" , "2" , "5" , "7" ,"10", "15", "20", "25", "30",
                 "35", "40")
window_size <- ordered(window_size, levels = c("roll_win_pct_diff" , "2" , "5" , "7" ,
                                      "10", "15", "20", "25", "30", "35", "40"))

plot_data <- data.frame(r_squared_window,r_squared_window_plus_sn_avg, window_size, diff)

plot_data %>% ggplot()+
  geom_point(y=r_squared_window, x=window_size, aes(color = "window"))+
  geom_point(y=r_squared_window_plus_sn_avg, x= window_size, aes(color = "window & season_avg"))+
  geom_point(y=diff, x= window_size, aes(color = "additional varriance"))+
  geom_line(aes(y=r_squared_window,x=window_size ), group = 1, color = "green")+
  geom_line(aes(y=r_squared_window_plus_sn_avg, x= window_size), group = 1,
            color = "darkturquoise")+
  geom_line(aes(y=diff, x= window_size), group = 1, color = "red")+
  labs(color = "Predictor",
       y= "r-squared",
       x ="Window size",
       caption= "* each subset of prior games got its own coeff",
       title = "Variance removed from response for windows of games")+
  theme_bw()


```
Simply using windows of previous games offers no benefit explaining the response.

### Temporal Weighting:
Larger windows can be improved by allowing them to assign different weights to 
depending on how close the current game of interest.

```{r}
# ## Window size 10 hollow
# summary(lm(score_diff ~ wind_2 + wind_5_hollow + wind_7_hollow + wind_10_hollow,
#            data = together_net_Rating_allWindows))
# summary(lm(score_diff ~ wind_2 + wind_5_hollow + wind_7_hollow + wind_10_hollow
#            + net_rating_diff ,
#            data = together_net_Rating_allWindows))
# # Adjusted R-squared: 0.1241  => 0.2305  
# 
# 
# ## Window size of 30 hollow
# summary(lm(score_diff ~ wind_5 + wind_10_hollow_5 + wind_15_hollow +
#             wind_20_hollow + wind_25_hollow + wind_30_hollow, data =
#         together_net_Rating_allWindows))
# summary(lm(score_diff ~ wind_5 + wind_10_hollow_5 + wind_15_hollow +
#           wind_20_hollow + wind_25_hollow + wind_30_hollow +
#           net_rating_diff , data = together_net_Rating_allWindows))
# #Adjusted R-squared:  0.1526 => 0.2324  
# 
# ## Window size of 35 hollow
# summary(lm(score_diff ~ wind_5 + wind_10_hollow_5 + wind_15_hollow +
#  wind_20_hollow + wind_25_hollow + wind_30_hollow + wind_35_hollow,
#  data = together_net_Rating_allWindows))
# summary(lm(score_diff ~ wind_5 + wind_10_hollow_5 + wind_15_hollow + wind_20_hollow +
#              wind_25_hollow + wind_30_hollow + wind_35_hollow + net_rating_diff ,
#           data = together_net_Rating_allWindows))
# # Adjusted R-squared:  0.1556  =>  0.2323  
# 
# # Window size of 40 hollow
# summary(lm(score_diff ~ wind_5 + wind_10_hollow_5 + wind_15_hollow + wind_20_hollow +
#          wind_25_hollow + wind_30_hollow + wind_35_hollow + wind_40_hollow,
#          data = together_net_Rating_allWindows))

summary(lm(score_diff ~ wind_5 + wind_10_hollow_5 +  wind_15_hollow + wind_20_hollow +
        wind_25_hollow + wind_30_hollow + wind_35_hollow + wind_40_hollow ,
         data = together_net_Rating_allWindows))
# # Adjusted R-squared:  0.1566  =>  0.2325  

r_squared_window <- c(0.1241 , 0.1526 , 0.1556 , 0.1566)
r_squared_window_plus_sn_avg <- c(0.2305, 0.2324, 0.2323, 0.2325)
window_size <- c("wind_10", "wind_30", "wind_35", "wind_40")
window_size <- ordered(window_size, levels = c("wind_10", "wind_30", "wind_35", 
                                               "wind_40"))
plot_data <- data.frame(r_squared_window,r_squared_window_plus_sn_avg, window_size)

plot_data %>% ggplot()+
    geom_hline(yintercept =0.2315, color = "darkgreen",alpha = .25, 
             linetype ="dashed", size =1.25)+
  geom_point(y=r_squared_window, x=window_size, aes(color = "window"))+
  geom_point(y=r_squared_window_plus_sn_avg, x= window_size,
             aes(color = "window & season_avg"))+
  geom_line(aes(y=r_squared_window,x=window_size ), group = 1, color = "red")+
  geom_line(aes(y=r_squared_window_plus_sn_avg, x= window_size), group = 1, 
            color = "darkturquoise")+
  labs(color = "Predictor",
       y= "r-squared",
       x ="Window size",
       title = "Variance removed from response by temporally weighted windows",
       caption = "green line is best prefroming non-temporal window & season avg")+
  theme_bw()

```
It is no help to use my temporal weighting idea.

---
## Ignore this section now it is no longer accurate 

#### Inside the model  
It is fair to ask if the model is actually assigning the temporal weights in the 
way I asserted would add predictive value. The coefficients here show how 
relevant a window of time is to the game outcome.
```{r echo = TRUE}
# Coefficients:
#                    Estimate Std. Error t value Pr(>|t|)    
# (Intercept)       -3.600e-16  9.095e-02   0.000  1.00000    
# wind_1_5          3.399e-02  1.976e-03  17.203  < 2e-16 ***
# wind_6_10         2.587e-02  2.082e-03  12.423  < 2e-16 ***
# wind_11_15        1.870e-02  2.173e-03   8.606  < 2e-16 ***
# wind_16_20        1.224e-02  2.273e-03   5.384 7.35e-08 ***
# wind_21_25        1.018e-02  2.360e-03   4.316 1.60e-05 ***
# wind_26_30        1.147e-02  2.481e-03   4.623 3.80e-06 ***
# wind_31_35        9.338e-03  2.589e-03   3.607  0.00031 ***
# wind_36_40        5.312e-03  2.634e-03   2.017  0.04376 *  
# net_rating_diff   2.763e-01  1.548e-02  17.849  < 2e-16 ***
# ---
```
The regression is in general doing what we expect by weighting more recent games 
more heavily since they are contributing more information about
the current state of the team.

### Decreasing relevance of the season average as observations increase:  

As the season progresses we get more observations of the team, so we want
to shift importance off the final season net rating onto the rolling 
weighted window. For example, when we have only seen the team play once or 
twice we want the season average rating to make up almost all of the prediction. 
However, once we have the weighted average of the past 40 games the 
season average should no longer make up nearly all of the prediction.
A simple linear model has fixed coefficients that cannot change in this 
way. So I made an interaction with the number games the team has played in 
the season so the model would make this tradeoff. This improved 
the performance of the model to an adjusted r-squared of 0.1654.  

```{r}

summary(lm(score_diff ~ wind_5 + wind_10_hollow_5 + wind_15_hollow + wind_20_hollow +
        wind_25_hollow + wind_30_hollow + wind_35_hollow + wind_40_hollow,
         data = together_net_Rating_allWindows))


# r_squared_window_plus_sn_avg <- c(0.1634, 0.1643, 0.1644, 0.1654)
# window_size <- c("wind_10", "wind_30", "wind_35", "wind_40", "wind_40_adj_sn")
# window_size <- ordered(window_size, levels = c("wind_10", "wind_30", "wind_35", 
#                                                "wind_40", "wind_40_adj_sn"))
# plot_data <- data.frame(r_squared_window_plus_sn_avg, window_size)
# 
# plot_data %>% ggplot(aes(y=r_squared_window_plus_sn_avg, x= window_size))+
#   geom_point()+
#   geom_line( group = 1)+
#   labs(color = "Predictor",
#        y= "r-squared",
#        x ="Window size",
#        caption= "* each subset of prior games got its own coeff",
#        title = "Variance removed from entire dataset")+
#   theme_bw()


```

Again looking inside the model we can see it is behaving exactly as expected as 
the season goes on the importance of the season average is decreased.
```{r echo = TRUE}
# Coefficients:
#                               Estimate Std. Error t value Pr(>|t|)    
# (Intercept)                  -2.807921   0.251202 -11.178  < 2e-16 ***
# wind_5                        0.034744   0.002723  12.757  < 2e-16 ***
# wind_10_hollow_5              0.024875   0.002868   8.672  < 2e-16 ***
# wind_15_hollow                0.019363   0.002994   6.467 1.05e-10 ***
# wind_20_hollow                0.014574   0.003137   4.646 3.43e-06 ***
# wind_25_hollow                0.009554   0.003265   2.927 0.003435 ** 
# wind_30_hollow                0.012856   0.003438   3.739 0.000186 ***
# wind_35_hollow                0.012233   0.003603   3.395 0.000689 ***
# wind_40_hollow                0.008635   0.003689   2.341 0.019257 *  
# net_rating_diff               0.380649   0.036473  10.436  < 2e-16 ***
# games_played                 -0.001283   0.005654  -0.227 0.820548    
# net_rating_diff:games_played -0.003210   0.000870  -3.690 0.000226 ***
```

### Unconstrained Centered Window: 
Now we are expanding model by using all of the previous techniques 
but also using the outcomes of future games to get a more accurate picture of 
how good the team is at the current point in time.  
```{r}
## Window size of 10 centered
summary(lm(score_diff ~ wind_10_center + net_rating_diff ,
           data = together_net_Rating_allWindows))
# Adjusted R-squared: 0.1241 => 0.1487
# 
# # Window size of 30 hollow_centered
summary(lm(game_net_rating ~ wind_10_center + hollow_wind20_cent +
             hollow_wind30_cent + net_rating_diff,
         data = together_net_Rating_allWindows))
# 
# ## Window size of 40 hollow_centered
summary(lm(game_net_rating ~ wind_10_center + hollow_wind20_cent +
          hollow_wind30_cent + hollow_wind40_cent + net_rating_diff,
          data = together_net_Rating_allWindows))
# 
# ## window size of 50 hollow_centered
summary(lm(game_net_rating ~ wind_10_center + hollow_wind20_cent + hollow_wind30_cent+
           hollow_wind40_cent + hollow_wind50_cent + net_rating_diff ,
           data = together_net_Rating_allWindows))
# 
# ## window size of 60 hollow_centered
summary(lm(game_net_rating ~ wind_10_center + hollow_wind20_cent + hollow_wind30_cent+
           hollow_wind40_cent + hollow_wind50_cent + hollow_wind60_cent +
             net_rating_diff,
           data = together_net_Rating_allWindows))
# 
# 
## window size of 60 hollow_centered full
summary(lm(game_net_rating ~ wind_10_center + hollow_wind20_cent + hollow_wind30_cent+
           hollow_wind40_cent + hollow_wind50_cent + hollow_wind60_cent + net_rating_diff +
           games_played * net_rating_diff,
           data = together_net_Rating_allWindows))



r_squared_window_plus_sn_avg <- c( 0.1577, 0.1777 , 0.181 , 0.1822 , 0.1845 )

window_size <- c("wind_10_future", "wind_30_future", "wind_40_future", "wind_50_future", 
                 "wind_60_future" )

window_size <- ordered(window_size, levels = c("wind_10_future", "wind_30_future", 
                      "wind_40_future", "wind_50_future", "wind_60_future" ))

plot_data <- data.frame(r_squared_window_plus_sn_avg, window_size)

plot_data %>% ggplot(aes(y=r_squared_window_plus_sn_avg, x= window_size))+
  geom_point()+
  geom_hline(yintercept = 0.1654, color = "darkgreen",alpha = .25, 
             linetype ="dashed", size =1.25)+
  geom_line( group = 1)+
  labs(color = "Predictor",
       y= "r-squared",
       x ="Window size",
       caption= "green line is best prefroming temporal window w/o using future games",
       title = "Variance removed from entire dataset")+
  theme_bw()

```

This allowed us to explain a little less than 2 percent additional variance in 
the response.



### 10-fold cross validation:
With a more complex model for team strength we needed to ensure we were not 
just overfitting to the data. The 60 game centered window was the best in 10-fold 
cross validation and will give us the best proxy for team strength.

```{r}
## Finding the best proxy for team strength 
set.seed(2015)

## Creating the test folds
together_net_Rating_allWindows <- together_net_Rating_allWindows %>%
  mutate(test_fold = sample(rep(1:10, length.out = n())))

## generates the holdout predictions
holdout_predictions <- 
  map_dfr(unique(together_net_Rating_allWindows$test_fold), 
          function(holdout) {
            # Separate test and training data:
            test_data <- together_net_Rating_allWindows %>% filter(test_fold == holdout)
            train_data <- together_net_Rating_allWindows %>% filter(test_fold != holdout)
            
            
            # Train models:
            ## mean alone
            mean_alone <- lm(game_net_rating ~ net_rating_diff , data = train_data)
            
            ## window 30
            w_30 <- lm(game_net_rating ~ wind_30 + net_rating_diff , data = train_data)
            
            ## window 35
            w_35 <- lm(game_net_rating ~ wind_35 + net_rating_diff , data = train_data)
            
            ## window 40
            w_40 <- lm(game_net_rating ~ wind_40 + net_rating_diff , data = train_data)
            
            ## window 30 temporal
            w30_temp <- lm(game_net_rating ~ wind_5 + wind_10_hollow_5 + wind_15_hollow + 
                           wind_20_hollow + wind_25_hollow + wind_30_hollow +
                           net_rating_diff , data = train_data)
            
            ## window 35 temporal
            w35_temp <- lm(game_net_rating ~ wind_5 + wind_10_hollow_5 + wind_15_hollow + 
                wind_20_hollow + wind_25_hollow + wind_30_hollow + wind_35_hollow +
                           net_rating_diff , data = train_data)
            
            ## window 40 temporal
            w40_temp <- lm(game_net_rating ~ wind_5 + wind_10_hollow_5 + wind_15_hollow + 
                wind_20_hollow + wind_25_hollow + wind_30_hollow + wind_35_hollow +
                  wind_40_hollow + net_rating_diff , data = train_data)
            
            ## Window 40 temporal weighted
            w40_tp <- lm(game_net_rating ~  wind_5 + wind_10_hollow_5 + wind_15_hollow +
            wind_20_hollow + wind_25_hollow + wind_30_hollow + wind_35_hollow +
            wind_40_hollow + net_rating_diff  + games_played * net_rating_diff,
            data = together_net_Rating_allWindows )
            
            ## Window 40 temporal centered
            w40_temp_cen <- lm(game_net_rating ~ wind_10_center + hollow_wind20_cent + 
                    hollow_wind30_cent + hollow_wind40_cent + net_rating_diff,
                    data = together_net_Rating_allWindows)
            
            ## Window 50 temporal centered
            w50_temp_cen <- lm(game_net_rating ~ wind_10_center + hollow_wind20_cent + 
                    hollow_wind30_cent + hollow_wind40_cent + hollow_wind50_cent +
                    net_rating_diff,
                    data = together_net_Rating_allWindows)
            
            ## Window 60 temporal centered
            w60_temp_cen <- lm(game_net_rating ~ wind_10_center + hollow_wind20_cent + 
                    hollow_wind30_cent + hollow_wind40_cent + hollow_wind50_cent +
                    hollow_wind60_cent + net_rating_diff,
                    data = together_net_Rating_allWindows)
            
            
            # Return tibble of holdout results:
            tibble(sn_avg = predict(mean_alone, newdata = test_data),
                   w_30 = predict(w_30, newdata = test_data),
                   w_35 = predict(w_35, newdata = test_data),
                   w_40 = predict(w_40, newdata = test_data),
                   w30_t = predict(w30_temp, newdata = test_data),
                   w35_t = predict(w35_temp, newdata = test_data),
                   w40_t = predict(w40_temp, newdata = test_data),
                   w40_tp = predict(w40_tp, newdata = test_data),
                   w40_tc = predict(w40_temp_cen, newdata = test_data),
                   w50_tc = predict(w50_temp_cen, newdata = test_data),
                   w60_tc = predict(w60_temp_cen, newdata = test_data),
                   test_actual = test_data$score_diff, 
                   test_fold = holdout) 
          })

holdout_predictions <- holdout_predictions %>%
  pivot_longer(sn_avg:w60_tc, 
               names_to = "type", values_to = "test_preds")

holdout_predictions$type <-  ordered(holdout_predictions$type, c("sn_avg","w_2","w_5",
                        "w_7", "w_10", "w_15", "w_20", "w_25","w_30","w_35", 
                        "w_40", "w30_t", "w35_t","w40_t","w40_tp", "w40_tc", 
                        "w50_tc", "w60_tc"))
holdout_predictions$test_fold <- as.factor(holdout_predictions$test_fold)


holdout_predictions %>% 
  group_by(type, test_fold) %>%
  summarize(rmse =
              sqrt(mean((test_actual - test_preds)^2))) %>% 
  ggplot(aes(x = type, y = rmse)) + 
  geom_point() + theme_bw() +
  stat_summary(fun = mean, geom = "point", 
               color = "red") + 
  stat_summary(fun.data = mean_se, geom = "errorbar",
               color = "red") +
  labs(title = "Game net rating difference as response",
       caption = "t = temporal weighting
       tc = temporal weighting and using future games
       tp = temporal weighting and dynamic season average",
       x = "model")
```



